{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import EfficientNet models with intermediate endpoints\nimport sys\nsys.path.append('/kaggle/input/efficientnetv2-head-1x1-endpoint-v2/')\nsys.path.append('/kaggle/input/efficientnetv2-head-1x1-endpoint-v2/efficientnetv2/')","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:53:50.862488Z","iopub.execute_input":"2022-09-16T15:53:50.862864Z","iopub.status.idle":"2022-09-16T15:53:50.892177Z","shell.execute_reply.started":"2022-09-16T15:53:50.862769Z","shell.execute_reply":"2022-09-16T15:53:50.891437Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\n\nfrom kaggle_datasets import KaggleDatasets\nfrom tqdm.notebook import tqdm\nfrom multiprocessing import cpu_count\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedKFold\nfrom albumentations import *\n\nimport effnetv2_model\nimport re\nimport os\nimport io\nimport time\nimport pickle\nimport math\nimport random\nimport sys\nimport cv2\nimport gc\n\nimport matplotlib as mpl\nmpl.rcParams['figure.dpi'] = 150\n\nprint(f'tensorflow version: {tf.__version__}')\nprint(f'tensorflow keras version: {tf.keras.__version__}')\nprint(f'python version: P{sys.version}')","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:53:50.893546Z","iopub.execute_input":"2022-09-16T15:53:50.893934Z","iopub.status.idle":"2022-09-16T15:53:59.731072Z","shell.execute_reply.started":"2022-09-16T15:53:50.893904Z","shell.execute_reply":"2022-09-16T15:53:59.729968Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2022-09-16 15:53:51.642556: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2022-09-16 15:53:51.642696: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","output_type":"stream"},{"name":"stdout","text":"tensorflow version: 2.4.1\ntensorflow keras version: 2.4.0\npython version: P3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 16:07:37) \n[GCC 9.3.0]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', TPU.master())\nexcept ValueError:\n    print('Running on GPU')\n    TPU = None\n\nif TPU:\n    IS_TPU = True\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    strategy = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    IS_TPU = False\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}, IS_TPU: {IS_TPU}')","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:53:59.732565Z","iopub.execute_input":"2022-09-16T15:53:59.732815Z","iopub.status.idle":"2022-09-16T15:54:06.141153Z","shell.execute_reply.started":"2022-09-16T15:53:59.732787Z","shell.execute_reply":"2022-09-16T15:54:06.140359Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Running on TPU  grpc://10.0.0.2:8470\n","output_type":"stream"},{"name":"stderr","text":"2022-09-16 15:53:59.741421: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2022-09-16 15:53:59.744992: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2022-09-16 15:53:59.745042: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n2022-09-16 15:53:59.745073: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (2f46f6f59634): /proc/driver/nvidia/version does not exist\n2022-09-16 15:53:59.749060: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-09-16 15:53:59.750580: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2022-09-16 15:53:59.788931: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2022-09-16 15:53:59.788998: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30020}\n2022-09-16 15:53:59.808067: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2022-09-16 15:53:59.808128: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30020}\n2022-09-16 15:53:59.809642: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:30020\n","output_type":"stream"},{"name":"stdout","text":"REPLICAS: 8, IS_TPU: True\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# For TPU's the dataset needs to be stored in Google Cloud\n# Retrieve the Google Cloud location of the dataset\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('hubmap-patched-tfrecords-300x300')","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:54:06.142769Z","iopub.execute_input":"2022-09-16T15:54:06.142993Z","iopub.status.idle":"2022-09-16T15:54:06.544526Z","shell.execute_reply.started":"2022-09-16T15:54:06.142967Z","shell.execute_reply":"2022-09-16T15:54:06.543821Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"SEED = 43\nDEBUG = False\n\n# Image dimensions\nIMG_SIZE_FULL = 640\nIMG_SIZE = 640\nN_PATCHES_PER_IMAGE = (IMG_SIZE_FULL // IMG_SIZE) ** 2\nN_CHANNELS = 3\nINPUT_SHAPE = (IMG_SIZE, IMG_SIZE, N_CHANNELS)\n\n# EfficientNet version, b0/b1/b2/b3/b4/b5/b6/b7/b8\nEFN_SIZE = 'b8'\n# Peak Learning Rate\nLR_MAX_WHOLE = 8e-4\nN_FOLDS = 4\nTRAIN_FOLDS = [0]\n\n# Epochs are run 10 at a time due to low training samples\nCOMBINE_EPOCHS = 4\nEPOCHS_WHOLE = 200 // COMBINE_EPOCHS\n\n# Batch size\nBATCH_SIZE = 8 * REPLICAS\n\n# Dataset Mean and Standard Deviation\nMEAN = np.load('/kaggle/input/hubmap-patched-tfrecords-300x300/MEAN.npy')\nSTD = np.load('/kaggle/input/hubmap-patched-tfrecords-300x300/STD.npy')\n\n# Tensorflow AUTO flag\nAUTO = tf.data.experimental.AUTOTUNE\nif TPU:\n    NUM_PARALLEL_CALLS = AUTO\nelse:\n    NUM_PARALLEL_CALLS = cpu_count()\n\nprint(f'BATCH_SIZE: {BATCH_SIZE}, NUM_PARALLEL_CALLS: {NUM_PARALLEL_CALLS}')","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:54:06.545797Z","iopub.execute_input":"2022-09-16T15:54:06.546566Z","iopub.status.idle":"2022-09-16T15:54:06.563657Z","shell.execute_reply.started":"2022-09-16T15:54:06.546527Z","shell.execute_reply":"2022-09-16T15:54:06.562759Z"},"trusted":true},"outputs":[{"name":"stdout","text":"BATCH_SIZE: 64, NUM_PARALLEL_CALLS: -1\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(f'MEAN: {MEAN}, STD: {STD}')","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:54:06.564749Z","iopub.execute_input":"2022-09-16T15:54:06.565380Z","iopub.status.idle":"2022-09-16T15:54:06.572339Z","shell.execute_reply.started":"2022-09-16T15:54:06.565349Z","shell.execute_reply":"2022-09-16T15:54:06.571331Z"},"trusted":true},"outputs":[{"name":"stdout","text":"MEAN: [0.16183016 0.18965785 0.1697808 ], STD: [0.1697331  0.19511677 0.175394  ]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Seed","metadata":{}},{"cell_type":"code","source":"# Seed all random number generators\ndef seed_everything(seed=SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \n\nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:54:06.573416Z","iopub.execute_input":"2022-09-16T15:54:06.574134Z","iopub.status.idle":"2022-09-16T15:54:06.583033Z","shell.execute_reply.started":"2022-09-16T15:54:06.574102Z","shell.execute_reply":"2022-09-16T15:54:06.582175Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"# Train DataFrame\ntrain = pd.read_csv('/kaggle/input/hubmap-organ-segmentation/train.csv')\nN_SAMPLES = len(train)\nprint(f'N_SAMPLES: {N_SAMPLES}')\n\n# Add Ordinal Encoded Organ\ntrain['organ_ordinal'] = train['organ'].astype('category').cat.codes\nN_ORGANS = train['organ'].nunique()\nORGANS = sorted(train['organ'].unique())\norg_ord2org = dict(enumerate(train['organ'].astype('category').cat.categories))\nprint(f'N_ORGANS: {N_ORGANS}, ORGANS: {ORGANS}')\n\ndisplay(train.head())\ndisplay(train.info())","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:54:06.584211Z","iopub.execute_input":"2022-09-16T15:54:06.584721Z","iopub.status.idle":"2022-09-16T15:54:07.023025Z","shell.execute_reply.started":"2022-09-16T15:54:06.584684Z","shell.execute_reply":"2022-09-16T15:54:07.022084Z"},"trusted":true},"outputs":[{"name":"stdout","text":"N_SAMPLES: 351\nN_ORGANS: 5, ORGANS: ['kidney', 'largeintestine', 'lung', 'prostate', 'spleen']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      id     organ data_source  img_height  img_width  pixel_size  \\\n0  10044  prostate         HPA        3000       3000         0.4   \n1  10274  prostate         HPA        3000       3000         0.4   \n2  10392    spleen         HPA        3000       3000         0.4   \n3  10488      lung         HPA        3000       3000         0.4   \n4  10610    spleen         HPA        3000       3000         0.4   \n\n   tissue_thickness                                                rle   age  \\\n0                 4  1459676 77 1462675 82 1465674 87 1468673 92 14...  37.0   \n1                 4  715707 2 718705 8 721703 11 724701 18 727692 3...  76.0   \n2                 4  1228631 20 1231629 24 1234624 40 1237623 47 12...  82.0   \n3                 4  3446519 15 3449517 17 3452514 20 3455510 24 34...  78.0   \n4                 4  478925 68 481909 87 484893 105 487863 154 4908...  21.0   \n\n      sex  organ_ordinal  \n0    Male              3  \n1    Male              3  \n2    Male              4  \n3    Male              2  \n4  Female              4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>organ</th>\n      <th>data_source</th>\n      <th>img_height</th>\n      <th>img_width</th>\n      <th>pixel_size</th>\n      <th>tissue_thickness</th>\n      <th>rle</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>organ_ordinal</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10044</td>\n      <td>prostate</td>\n      <td>HPA</td>\n      <td>3000</td>\n      <td>3000</td>\n      <td>0.4</td>\n      <td>4</td>\n      <td>1459676 77 1462675 82 1465674 87 1468673 92 14...</td>\n      <td>37.0</td>\n      <td>Male</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10274</td>\n      <td>prostate</td>\n      <td>HPA</td>\n      <td>3000</td>\n      <td>3000</td>\n      <td>0.4</td>\n      <td>4</td>\n      <td>715707 2 718705 8 721703 11 724701 18 727692 3...</td>\n      <td>76.0</td>\n      <td>Male</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10392</td>\n      <td>spleen</td>\n      <td>HPA</td>\n      <td>3000</td>\n      <td>3000</td>\n      <td>0.4</td>\n      <td>4</td>\n      <td>1228631 20 1231629 24 1234624 40 1237623 47 12...</td>\n      <td>82.0</td>\n      <td>Male</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10488</td>\n      <td>lung</td>\n      <td>HPA</td>\n      <td>3000</td>\n      <td>3000</td>\n      <td>0.4</td>\n      <td>4</td>\n      <td>3446519 15 3449517 17 3452514 20 3455510 24 34...</td>\n      <td>78.0</td>\n      <td>Male</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10610</td>\n      <td>spleen</td>\n      <td>HPA</td>\n      <td>3000</td>\n      <td>3000</td>\n      <td>0.4</td>\n      <td>4</td>\n      <td>478925 68 481909 87 484893 105 487863 154 4908...</td>\n      <td>21.0</td>\n      <td>Female</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 351 entries, 0 to 350\nData columns (total 11 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   id                351 non-null    int64  \n 1   organ             351 non-null    object \n 2   data_source       351 non-null    object \n 3   img_height        351 non-null    int64  \n 4   img_width         351 non-null    int64  \n 5   pixel_size        351 non-null    float64\n 6   tissue_thickness  351 non-null    int64  \n 7   rle               351 non-null    object \n 8   age               351 non-null    float64\n 9   sex               351 non-null    object \n 10  organ_ordinal     351 non-null    int8   \ndtypes: float64(2), int64(4), int8(1), object(4)\nmemory usage: 27.9+ KB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# Utility Functions","metadata":{}},{"cell_type":"code","source":"# One in K chance function\ndef tf_rand_int(minval, maxval, dtype=tf.int64):\n    minval = tf.cast(minval, dtype)\n    maxval = tf.cast(maxval, dtype)\n    return tf.random.uniform(shape=(), minval=minval, maxval=maxval, dtype=dtype)\n\ndef one_in(k):\n    return 0 == tf_rand_int(0, k)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:54:07.024701Z","iopub.execute_input":"2022-09-16T15:54:07.025251Z","iopub.status.idle":"2022-09-16T15:54:07.032489Z","shell.execute_reply.started":"2022-09-16T15:54:07.025206Z","shell.execute_reply":"2022-09-16T15:54:07.031603Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Loss\n\nThe following code provides a wide variety of semantic segmentations losses and metrics. Surprisingly, dice loss did perform worse than good old binary crossnetropy. The only code used in this notebook is the IoU metric.","metadata":{}},{"cell_type":"code","source":"# Source: https://github.com/shruti-jadon/Semantic-Segmentation-Loss-Functions/blob/master/loss_functions.py\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.losses import binary_crossentropy\n\nbeta = 0.25\nalpha = 0.25\ngamma = 2\nepsilon = 1e-5\nsmooth = 1\nthreshold=0.50\n\nclass Semantic_loss_functions(object):\n    def __init__(self):\n        print ('semantic loss functions initialized')\n        \n    def iou(self, y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.where(y_pred > threshold, x=1.0, y=0.0)\n        y_true_f = K.flatten(y_true)\n        y_pred_f = K.flatten(y_pred)\n        intersection = K.sum(y_true_f * y_pred_f)\n        union = K.sum(y_true_f + y_pred_f) - intersection\n        return intersection / (union + epsilon)\n\n    def dice_coef(self, y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32)\n        y_true_f = K.flatten(y_true)\n        y_pred_f = K.flatten(y_pred)\n        intersection = K.sum(y_true_f * y_pred_f, axis=1)\n        return (2. * intersection + K.epsilon()) / (\n                    K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon())\n\n    def sensitivity(self, y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        return true_positives / (possible_positives + K.epsilon())\n\n    def specificity(self, y_true, y_pred):\n        true_negatives = K.sum(\n            K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n        possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n        return true_negatives / (possible_negatives + K.epsilon())\n\n    def convert_to_logits(self, y_pred):\n        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(),\n                                  1 - tf.keras.backend.epsilon())\n        return tf.math.log(y_pred / (1 - y_pred))\n\n    def weighted_cross_entropyloss(self, y_true, y_pred):\n        y_pred = self.convert_to_logits(y_pred)\n        pos_weight = beta / (1 - beta)\n        loss = tf.nn.weighted_cross_entropy_with_logits(logits=y_pred,\n                                                        targets=y_true,\n                                                        pos_weight=pos_weight)\n        return tf.reduce_mean(loss)\n\n    def focal_loss_with_logits(self, logits, targets, alpha, gamma, y_pred):\n        weight_a = alpha * (1 - y_pred) ** gamma * targets\n        weight_b = (1 - alpha) * y_pred ** gamma * (1 - targets)\n\n        return (tf.math.log1p(tf.exp(-tf.abs(logits))) + tf.nn.relu(\n            -logits)) * (weight_a + weight_b) + logits * weight_b\n\n    def focal_loss(self, y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(),\n                                  1 - tf.keras.backend.epsilon())\n        logits = tf.math.log(y_pred / (1 - y_pred))\n\n        loss = self.focal_loss_with_logits(logits=logits, targets=y_true,\n                                      alpha=alpha, gamma=gamma, y_pred=y_pred)\n\n        return tf.reduce_mean(loss)\n\n    def depth_softmax(self, matrix):\n        sigmoid = lambda x: 1 / (1 + K.exp(-x))\n        sigmoided_matrix = sigmoid(matrix)\n        softmax_matrix = sigmoided_matrix / K.sum(sigmoided_matrix, axis=0)\n        return softmax_matrix\n\n    def generalized_dice_coefficient(self, y_true, y_pred):\n        smooth = 1e0\n        y_true = tf.cast(y_true, tf.float32)\n        y_true_f = K.flatten(y_true)\n        y_pred_f = K.flatten(y_pred)\n        intersection = K.sum(y_true_f * y_pred_f)\n        return (2. * intersection + smooth) / (\n                    K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n        \n    def dice_loss(self, y_true, y_pred):\n        loss = 1 - self.generalized_dice_coefficient(y_true, y_pred)\n        return loss\n    \n    def dice_loss_symmetric(self, y_true, y_pred):\n        loss = 1 - self.generalized_dice_coefficient(y_true, y_pred)\n        loss_neg = 1 - self.generalized_dice_coefficient(1 - y_true, 1 - y_pred)\n        return 0.50 * (loss + loss_neg)\n\n    def bce_dice_loss(self, y_true, y_pred):\n        loss = binary_crossentropy(y_true, y_pred) + \\\n               self.dice_loss(y_true, y_pred)\n        return loss / 2.0\n\n    def confusion(self, y_true, y_pred):\n        smooth = 1\n        y_pred_pos = K.clip(y_pred, 0, 1)\n        y_pred_neg = 1 - y_pred_pos\n        y_pos = K.clip(y_true, 0, 1)\n        y_neg = 1 - y_pos\n        tp = K.sum(y_pos * y_pred_pos)\n        fp = K.sum(y_neg * y_pred_pos)\n        fn = K.sum(y_pos * y_pred_neg)\n        prec = (tp + smooth) / (tp + fp + smooth)\n        recall = (tp + smooth) / (tp + fn + smooth)\n        return prec, recall\n\n    def true_positive(self, y_true, y_pred):\n        smooth = 1\n        y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n        y_pos = K.round(K.clip(y_true, 0, 1))\n        tp = (K.sum(y_pos * y_pred_pos) + smooth) / (K.sum(y_pos) + smooth)\n        return tp\n\n    def true_negative(self, y_true, y_pred):\n        smooth = 1\n        y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n        y_pred_neg = 1 - y_pred_pos\n        y_pos = K.round(K.clip(y_true, 0, 1))\n        y_neg = 1 - y_pos\n        tn = (K.sum(y_neg * y_pred_neg) + smooth) / (K.sum(y_neg) + smooth)\n        return tn\n\n    def tversky_index(self, y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32)\n        y_true_pos = K.flatten(y_true)\n        y_pred_pos = K.flatten(y_pred)\n        true_pos = K.sum(y_true_pos * y_pred_pos)\n        false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n        false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n        alpha = 0.75\n        return (true_pos + smooth) / (true_pos + alpha * false_neg + (\n                    1 - alpha) * false_pos + smooth)\n\n    def tversky_loss(self, y_true, y_pred):\n        return 1 - self.tversky_index(y_true, y_pred)\n\n    def focal_tversky(self, y_true, y_pred, gamme=2.0):\n        pt_1 = self.tversky_index(y_true, y_pred)\n        return K.pow((1 - pt_1), gamma)\n\n    def log_cosh_dice_loss(self, y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32)\n        x = self.dice_loss(y_true, y_pred)\n        return tf.math.log((tf.exp(x) + tf.exp(-x)) / 2.0)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:54:07.037162Z","iopub.execute_input":"2022-09-16T15:54:07.037417Z","iopub.status.idle":"2022-09-16T15:54:07.073496Z","shell.execute_reply.started":"2022-09-16T15:54:07.037386Z","shell.execute_reply":"2022-09-16T15:54:07.072599Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"SEMANTIC_LOSS_FUNCTIONS = Semantic_loss_functions()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:54:07.075196Z","iopub.execute_input":"2022-09-16T15:54:07.076310Z","iopub.status.idle":"2022-09-16T15:54:07.090289Z","shell.execute_reply.started":"2022-09-16T15:54:07.076261Z","shell.execute_reply":"2022-09-16T15:54:07.089389Z"},"trusted":true},"outputs":[{"name":"stdout","text":"semantic loss functions initialized\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Feature Pyramid Network (FPN)\n\n(Pytorch Source)[https://www.kaggle.com/code/thedevastator/training-fastai-baseline#Model}","metadata":{}},{"cell_type":"code","source":"def FPN(xs, output_channels, last_layer, debug=False):\n    def _conv(x):\n        x = tf.keras.layers.ZeroPadding2D(padding=1)(x)\n        x = tf.keras.layers.Conv2D(output_channels * 2, 3, padding='SAME', kernel_initializer='he_normal', activation='relu')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.ZeroPadding2D(padding=1)(x)\n        x = tf.keras.layers.Conv2D(output_channels, 3, padding='SAME', kernel_initializer='he_normal')(x)\n        x = tf.image.resize(x, size=target_size, method=tf.image.ResizeMethod.BILINEAR)\n        x = tf.nn.relu(x)\n        return x\n\n    target_size = last_layer.shape[1:3]\n    xs = tf.keras.layers.Concatenate()([_conv(x) for x in xs])\n    x = tf.keras.layers.Concatenate()([xs, last_layer])\n\n    if debug:\n        return x, xs\n    else:\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:54:07.091385Z","iopub.execute_input":"2022-09-16T15:54:07.091613Z","iopub.status.idle":"2022-09-16T15:54:07.102532Z","shell.execute_reply.started":"2022-09-16T15:54:07.091587Z","shell.execute_reply":"2022-09-16T15:54:07.101655Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Atrous Spatial Pyramid Pooling (ASPP)\n\n(Pytorch Source)[https://www.kaggle.com/code/thedevastator/training-fastai-baseline#Model]","metadata":{}},{"cell_type":"code","source":"def ASPP(x, mid_c=320, dilations=[1, 2, 3, 4], out_c=640, debug=False):\n    def _aspp_module(x, filters, kernel_size, padding, dilation, groups=1):\n        x = tf.keras.layers.ZeroPadding2D(padding=padding)(x)\n        x = tf.keras.layers.Conv2D(\n                filters=filters,\n                kernel_size=kernel_size,\n                dilation_rate=dilation,\n                groups=1 if IS_TPU else groups,\n                kernel_initializer='he_uniform',\n            )(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.nn.relu(x)\n        \n        return x\n    \n    x0 = tf.math.reduce_max(x, axis=(1,2), keepdims=True)\n    x0 = tf.keras.layers.Conv2D(filters=mid_c, kernel_size=1, strides=1, kernel_initializer='he_uniform', use_bias=False)(x0)\n    x0 = tf.keras.layers.BatchNormalization(gamma_initializer=tf.constant_initializer(value=0.25))(x0)\n    x0 = tf.nn.relu(x0)\n                                  \n                                  \n    xs = (\n        [_aspp_module(x, mid_c, 1, padding=0, dilation=1)] +\n        [_aspp_module(x, mid_c, 3, padding=d, dilation=d, groups=4) for d in dilations]\n    )\n    \n    x0= tf.image.resize(x0, size=xs[0].shape[1:3])\n    x = tf.keras.layers.Concatenate()([x0] + xs)\n    x = tf.keras.layers.Conv2D(filters=out_c, kernel_size=1, kernel_initializer='he_uniform', use_bias=False)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.nn.relu(x)\n                       \n    if debug:\n        return x, x0, xs\n    else:\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:54:07.103927Z","iopub.execute_input":"2022-09-16T15:54:07.104549Z","iopub.status.idle":"2022-09-16T15:54:07.118293Z","shell.execute_reply.started":"2022-09-16T15:54:07.104515Z","shell.execute_reply":"2022-09-16T15:54:07.117247Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Upsample","metadata":{}},{"cell_type":"code","source":"# PixelShuffle implemented in Tensorflow, not used\ndef PixelShuffle(x, upscale_factor=2):\n    _, w, h, c = x.shape\n    n = -1\n\n    c_out = c // upscale_factor ** 2\n    w_out = w * upscale_factor\n    h_out = h * upscale_factor\n\n    x = tf.reshape(x, [-1, upscale_factor, upscale_factor, w, h, c_out])\n    x = tf.transpose(x, [0, 3, 1, 4, 2, 5])\n    x = tf.reshape(x, [-1, w_out, h_out, c_out])\n\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:54:07.119580Z","iopub.execute_input":"2022-09-16T15:54:07.120365Z","iopub.status.idle":"2022-09-16T15:54:07.133939Z","shell.execute_reply.started":"2022-09-16T15:54:07.120323Z","shell.execute_reply":"2022-09-16T15:54:07.132962Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Inspiration: https://www.tensorflow.org/tutorials/generative/pix2pix#build_an_input_pipeline_with_tfdata\ndef upsample(x, concat, target_filters, name, conv2dt_kernel_init_max, relu=True, dropout=0, debug=False):\n    filters = concat.shape[-1]\n    x_up = tf.keras.layers.Conv2DTranspose(\n            filters, # Number of Convolutional Filters\n            kernel_size=4, # Kernel Size\n            strides=2, # Kernel Steps\n            padding='SAME', # linear scaling\n            name=f'Conv2DTranspose_{name}', # Name of Layer\n            kernel_initializer='he_uniform',\n            use_bias=False,\n        )(x)\n    \n    concat = tf.keras.layers.BatchNormalization(\n        gamma_initializer=tf.constant_initializer(value=0.25),\n        name=f'BatchNormalization_{name}'\n    )(concat)\n    x = tf.keras.layers.Concatenate(name=f'Concatenate_{name}')([x_up, concat])\n    x = tf.nn.relu(x)\n    \n        \n    x = tf.keras.layers.Conv2D(target_filters, 3, padding='SAME', kernel_initializer='he_uniform', activation='relu', name=f'Conv2D_1_{name}')(x)\n    x = tf.keras.layers.Conv2D(target_filters, 3, padding='SAME', kernel_initializer='he_uniform', name=f'Conv2D_2_{name}')(x)\n    \n    if relu:\n        x = tf.nn.relu(x)\n    \n    x = tf.keras.layers.Dropout(dropout, name=f'Dropout_{name}')(x)\n\n    if debug:\n        return x, x_up, concat\n    else:\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:54:07.135078Z","iopub.execute_input":"2022-09-16T15:54:07.135503Z","iopub.status.idle":"2022-09-16T15:54:07.146844Z","shell.execute_reply.started":"2022-09-16T15:54:07.135474Z","shell.execute_reply":"2022-09-16T15:54:07.146047Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Model\n\nA lot of debug code is present in the model construction, which will be later used to show the values during a forward pass. ","metadata":{}},{"cell_type":"code","source":"GCS_WEIGHTS_PATH = KaggleDatasets().get_gcs_path('efficientnetv2-head-1x1-endpoint-v2')","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:54:07.148072Z","iopub.execute_input":"2022-09-16T15:54:07.148766Z","iopub.status.idle":"2022-09-16T15:54:07.539399Z","shell.execute_reply.started":"2022-09-16T15:54:07.148734Z","shell.execute_reply":"2022-09-16T15:54:07.538739Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def get_model(dropout_decoder=0, dropout_cnn=0, file_path=None, lr=1e-3, eps=1e-7, clipnorm=5.0, wd_coef=1e-2, cnn_trainable=True, debug=DEBUG):\n    # enable XLA optmizations\n    tf.config.optimizer.set_jit(True)\n    # Set seed for deterministic weights initialization\n    seed_everything()\n    \n    with strategy.scope():\n        # EfficientNetV2 Backbone # \n        cnn = effnetv2_model.get_model(f'efficientnet-{EFN_SIZE}', include_top=False, weights=None if IS_TPU else 'jft', model_config={ 'conv_dropout': dropout_cnn })\n        if IS_TPU:\n            WEIGHT_PATH = f'{GCS_WEIGHTS_PATH}/noisy_student_efficientnet-{EFN_SIZE}'\n            ckpt = tf.train.latest_checkpoint(WEIGHT_PATH)\n            cnn.load_weights(ckpt)\n\n        # Inputs, note the names are equal to the dictionary keys in the dataset\n        image = tf.keras.layers.Input(INPUT_SHAPE, name='image', dtype=tf.float32)\n        image_norm = tf.cast(image, tf.float32) / 255\n        image_norm = tf.keras.layers.experimental.preprocessing.Normalization(mean=MEAN, variance=STD, dtype=tf.float32)(image_norm)\n\n        embedding, up6, up5, up4, up3, up2, up1 = cnn(image_norm, with_endpoints=True)\n        \n                \n        dec0 = ASPP(up2)\n        \n        dec0 = tf.keras.layers.Dropout(0.30)(dec0)\n\n\n        dec1 = upsample(dec0, up3, up4.shape[-1] * 4, 'upsample1', 0.02, dropout=dropout_decoder)\n        dec2 = upsample(dec1, up4, up5.shape[-1] * 2, 'upsample2', 0.02, dropout=dropout_decoder)\n        dec3 = upsample(dec2, up5, up6.shape[-1] * 2, 'upsample3', 0.02)\n        dec4 = upsample(dec3, up6, 64, 'upsample4', 0.02)\n        \n        dec_fpn = FPN([dec0, dec1, dec2, dec3], 32, dec4)\n        \n\n        # Head\n        x = tf.keras.layers.Dropout(0.10)(dec_fpn)\n        x = tf.keras.layers.Conv2D(\n            filters=1,\n            kernel_size=1,\n            padding='SAME',\n            kernel_initializer=tf.random_normal_initializer(0.00, 0.05),\n            activation=None if debug else 'sigmoid',\n            name='Conv2D_3_head'\n        )(x)\n        output = tf.image.resize(x, size=[IMG_SIZE, IMG_SIZE], method=tf.image.ResizeMethod.BILINEAR)\n\n        # We will use the famous Adam optimizer for fast learning\n        optimizer = tf.optimizers.Adam(learning_rate=lr, epsilon=eps, clipnorm=clipnorm)\n\n        # Loss\n        loss = tf.keras.losses.BinaryCrossentropy()\n        \n        # Metrics\n        metrics = [\n            SEMANTIC_LOSS_FUNCTIONS.iou,\n            tf.keras.metrics.Precision(),\n            tf.keras.metrics.Recall(),\n            tf.keras.metrics.AUC(),\n            tf.keras.metrics.BinaryAccuracy(),\n        ]\n\n\n\n        model = tf.keras.models.Model(inputs=image, outputs=[output])\n        \n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n        if file_path:\n            print('Loading pretrained weights...')\n            model.load_weights(file_path)\n\n        return model","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:54:07.540842Z","iopub.execute_input":"2022-09-16T15:54:07.541114Z","iopub.status.idle":"2022-09-16T15:54:07.558069Z","shell.execute_reply.started":"2022-09-16T15:54:07.541083Z","shell.execute_reply":"2022-09-16T15:54:07.557224Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Pretrained File Path: '/kaggle/input/sartorius-training-dataset/model.h5'\ntf.keras.backend.clear_session()\ngc.collect()\n    \nmodel = get_model(file_path=None, debug=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:54:07.559384Z","iopub.execute_input":"2022-09-16T15:54:07.560821Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot model summary\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True, show_layer_names=True, expand_nested=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset and Augmentations","metadata":{}},{"cell_type":"code","source":"# Function to benchmark the dataset\ndef benchmark_dataset(dataset, num_epochs=3, n_steps_per_epoch=10, bs=BATCH_SIZE):\n    start_time = time.perf_counter()\n    for epoch_num in range(num_epochs):\n        for idx, (images, labels) in enumerate(dataset.take(n_steps_per_epoch + 1)):\n            if idx == 0:\n                epoch_start = time.perf_counter()\n            elif idx == 1 and epoch_num == 0:\n                print(f'image shape: {images.shape}, image dtype: {images.dtype}')\n            else:\n                pass\n        epoch_t = time.perf_counter() - epoch_start\n        mean_step_t = round(epoch_t / n_steps_per_epoch * 1000, 1)\n        n_imgs_per_s = int(1 / (mean_step_t / 1000) * bs)\n        print(f'epoch {epoch_num} took: {round(epoch_t, 2)} sec, mean step duration: {mean_step_t}ms, images/s: {n_imgs_per_s}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plots a batch of images\ndef show_batch(dataset, rows=4, cols=4):\n    imgs, lbls = next(iter(dataset))\n    imgs = imgs.numpy()\n    # Plot\n    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(rows*4, cols*4))\n    for r in range(rows):\n        for c in range(cols // 2):\n            img = imgs[r*cols+c]\n            axes[r, c*2].imshow(img)\n            axes[r, c*2].set_title(f'std: {img.std():.1f}')\n            lbl = lbls[r*cols+c]\n            axes[r, c*2+1].imshow(lbl)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decodes the TFRecords\ndef decode_image(record_bytes, val):\n    features = tf.io.parse_single_example(record_bytes, {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'mask': tf.io.FixedLenFeature([], tf.string),\n        'organ': tf.io.FixedLenFeature([], tf.string),\n    })\n\n    image = tf.io.parse_tensor(features['image'], out_type=tf.uint8)\n    image = tf.reshape(image, [IMG_SIZE, IMG_SIZE, N_CHANNELS])\n       \n    mask = tf.io.parse_tensor(features['mask'], out_type=tf.uint8)\n    mask = tf.reshape(mask, [IMG_SIZE, IMG_SIZE, 1])\n    \n    # Ogran\n    organ = features['organ']\n    \n    return image, mask, organ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def augment_image(image, mask, organ, val, return_organ):\n    \n    if not val:\n        rotations = tf_rand_int(0, 4, dtype=tf.int32)\n\n        image = tf.image.rot90(image, rotations)\n        mask = tf.image.rot90(mask, rotations)\n        \n        if one_in(2):\n            image = tf.image.transpose(image)\n            mask = tf.image.transpose(mask)\n        \n        # Pixel Level Augmentations\n        if one_in(2):\n            image = tf.image.random_hue(image, 0.2)\n        if one_in(2):\n            image = tf.image.random_saturation(image, 0.80, 1.20)\n        if one_in(2):\n            image = tf.image.random_contrast(image, 0.80, 1.20)\n        if one_in(2):\n            image = tf.image.random_brightness(image, 0.10)\n        if one_in(2):\n            image = tf.image.random_jpeg_quality(image, 75, 100)\n        \n        # Random Crop\n        offset_x = tf.random.uniform([], 0, tf.cast(IMG_SIZE * 0.50, tf.int32), dtype=tf.int32)\n        img_size_crop = IMG_SIZE - offset_x\n        if offset_x > 0:\n            offset_y = tf.random.uniform([], 0, offset_x, dtype=tf.int32)\n        else:\n            offset_y = tf.constant(0, dtype=tf.int32)\n\n        # Crop\n        if one_in(2):\n            image = tf.slice(image, [offset_x, offset_y, 0], [img_size_crop, img_size_crop, N_CHANNELS])\n            mask = tf.slice(mask, [offset_x, offset_y, 0], [img_size_crop, img_size_crop, 1])\n            \n            image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE], method=tf.image.ResizeMethod.BICUBIC)\n            image = tf.cast(image / tf.reduce_max(image) * 255, tf.uint8)\n            \n            mask = tf.image.resize(mask, [IMG_SIZE, IMG_SIZE], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n        \n        # Rotate\n        if one_in(2):\n            angle = tf.random.uniform([], -45 * math.pi / 180, 45 * math.pi / 180, dtype=tf.float32)\n            image = tfa.image.rotate(image, angle, interpolation='bilinear', fill_mode='reflect')\n            mask = tfa.image.rotate(mask, angle, interpolation='nearest', fill_mode='reflect')\n        \n        # Resize\n        image = tf.cast(image / tf.reduce_max(image) * 255, tf.uint8)\n        \n        # Explicit Reshape for TPU\n        image = tf.reshape(image, [IMG_SIZE, IMG_SIZE, N_CHANNELS])\n        mask = tf.reshape(mask, [IMG_SIZE, IMG_SIZE, 1])\n\n    if return_organ:\n        return image, mask, organ\n    else:\n        return image, mask","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('hubmap-patched-tfrecords-300x300')\nprint(f'GCS_DS_PATH: {GCS_DS_PATH}')\n\nTFRECORDS_FILE_PATHS = np.array(tf.io.gfile.glob(f'{GCS_DS_PATH}/*.tfrecords'))\nTFRECORDS_FILE_PATHS = np.array(\n    sorted(TFRECORDS_FILE_PATHS, key=lambda fp: int(fp.split('.')[-2].split('_')[-1]))\n)\nprint(f'Found {len(TFRECORDS_FILE_PATHS)} TFRecords')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check whether file paths are correctly ordered\npd.options.display.max_colwidth = 999\ndisplay(pd.DataFrame(TFRECORDS_FILE_PATHS, columns=['File Path']).sample(10, random_state=SEED))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"N_SAMPLES_PER_TFRECORD = np.load('/kaggle/input/hubmap-patched-tfrecords-300x300/N_SAMPLES_PER_TFRECORD.npy')\nprint(f'N_SAMPLES_PER_TFRECORD shape: {N_SAMPLES_PER_TFRECORD.shape}, dtype: {N_SAMPLES_PER_TFRECORD.dtype}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_dataset(tfrecord_idxs=None, bs=BATCH_SIZE, idxs=None, return_steps=False, val=False, return_organ=False, debug=False):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False\n    if IS_TPU:\n        npr = 1 if val else AUTO\n    else:\n        npr = 1 if val else cpu_count()\n    \n    if tfrecord_idxs is None:\n        dataset = tf.data.TFRecordDataset(TFRECORDS_FILE_PATHS, num_parallel_reads=npr)\n    else:\n        dataset = tf.data.TFRecordDataset(TFRECORDS_FILE_PATHS[tfrecord_idxs], num_parallel_reads=npr)\n    \n    dataset = dataset.map(\n        lambda record_bytes: decode_image(record_bytes, val), num_parallel_calls=AUTO if IS_TPU else 1\n    )\n    \n    # Cache dataset to speedup dataloader\n    dataset = dataset.cache()\n    \n    if not val and not debug:\n        dataset = dataset.with_options(ignore_order)\n        dataset = dataset.shuffle(128)\n        dataset = dataset.repeat()\n    \n    # Augment\n    dataset = dataset.map(\n        lambda image, mask, organ: augment_image(image, mask, organ, val, return_organ), num_parallel_calls=npr\n    )\n\n    dataset = dataset.batch(bs, drop_remainder=True)\n    dataset = dataset.prefetch(AUTO)\n    \n    if return_steps:\n        return dataset, math.ceil(N_SAMPLES_PER_TFRECORD[tfrecord_idxs].sum() / bs)\n    else:\n        return dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Benchmark Dataset\nbenchmark_dataset(get_dataset(debug=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sanity check\nimages, labels = next(iter(get_dataset(debug=True)))\nprint(f'images shape: {images.shape}, labels shape: {labels.shape}')\nprint(f'images dtype: {images.dtype}, labels dtype: {labels.dtype}')\n\npercentiles = [0.01, 0.05, 0.10, 0.25, 0.40 ,0.50, 0.60, 0.75, 0.90, 0.95, 0.99]\ndisplay(pd.Series(images.numpy().flatten()).describe(percentiles=percentiles).to_frame(name='images').T)\ndisplay(pd.Series(labels.numpy().flatten()).value_counts().to_frame(name='labels').T)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show what we will be training on\nshow_batch(get_dataset(bs=16, debug=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Learning Rate Scheduler\n\nAn learning rate scheduler is used with a cosine decay and exponential warmup.","metadata":{}},{"cell_type":"code","source":"def lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=EPOCHS_WHOLE):\n    \n    if current_step < num_warmup_steps:\n        return lr_max * 0.50 ** (num_warmup_steps - current_step)\n    else:\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_lr_schedule(lr_schedule, epochs):\n    fig = plt.figure(figsize=(20, 10))\n    plt.plot([None] + lr_schedule + [None])\n    # X Labels\n    x = np.arange(1, epochs + 1)\n    x_axis_labels = [i if epochs <= 40 or i % 5 == 0 or i == 1 else None for i in range(1, epochs + 1)]\n    plt.xlim([1, epochs])\n    plt.xticks(x, x_axis_labels) # set tick step to 1 and let x axis start at 1\n    \n    # Increase y-limit for better readability\n    plt.ylim([0, max(lr_schedule) * 1.1])\n    \n    # Title\n    schedule_info = f'start: {lr_schedule[0]:.1E}, max: {max(lr_schedule):.1E}, final: {lr_schedule[-1]:.1E}'\n    plt.title(f'Step Learning Rate Schedule, {schedule_info}', size=18, pad=12)\n    \n    # Plot Learning Rates\n    for x, val in enumerate(lr_schedule):\n        if epochs <= 40 or x % 5 == 0 or x is epochs - 1:\n            if x < len(lr_schedule) - 1:\n                if lr_schedule[x - 1] < val:\n                    ha = 'right'\n                else:\n                    ha = 'left'\n            elif x == 0:\n                ha = 'right'\n            else:\n                ha = 'left'\n            plt.plot(x + 1, val, 'o', color='black');\n            offset_y = (max(lr_schedule) - min(lr_schedule)) * 0.02\n            plt.annotate(f'{val:.1E}', xy=(x + 1, val + offset_y), size=12, ha=ha)\n    \n    plt.xlabel('Epoch', size=16, labelpad=5)\n    plt.ylabel('Learning Rate', size=16, labelpad=5)\n    plt.grid()\n    plt.show()\n\n# Learning rate for encoder\nLR_SCHEDULE = [lrfn(step, num_warmup_steps=3, lr_max=LR_MAX_WHOLE, num_cycles=0.50) for step in range(EPOCHS_WHOLE)]\nplot_lr_schedule(LR_SCHEDULE, epochs=EPOCHS_WHOLE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Whole Model","metadata":{}},{"cell_type":"code","source":"ORGAN_PER_TFRECORD = np.load('../input/hubmap-patched-tfrecords-300x300/ORGAN_PER_TFRECORD.npy')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Learning Rate Callback\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create KFOLDS\nFOLDS = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\nHISTORIES = dict()\nIDXS = { 'train_idxs': [], 'val_idxs': [] }\nfor fold, (train_idxs, val_idxs) in enumerate(FOLDS.split(X=TFRECORDS_FILE_PATHS, y=ORGAN_PER_TFRECORD)):\n    # Only train selected folds\n    if fold not in TRAIN_FOLDS:\n        continue\n    \n    IDXS['train_idxs'].append(train_idxs)\n    IDXS['val_idxs'].append(val_idxs)\n    train_dataset, train_steps_per_epoch = get_dataset(tfrecord_idxs=train_idxs, bs=BATCH_SIZE, return_steps=True)\n    val_dataset, val_steps_per_epoch = get_dataset(tfrecord_idxs=val_idxs, bs=N_PATCHES_PER_IMAGE * REPLICAS, return_steps=True, val=True)\n    print('=' * 80)\n    print(f'FOLD {fold}, train_steps_per_epoch: {train_steps_per_epoch}, val_steps_per_epoch: {val_steps_per_epoch}')\n    print('=' * 80)\n    \n    tf.keras.backend.clear_session()\n    gc.collect()\n    \n    model = get_model(file_path=None, cnn_trainable=True, lr=LR_MAX_WHOLE, eps=1e-5)\n    #model = get_model(file_path='../input/hubmap-training-tf-tpu-efficientnet-b8-640640-p/model_0.h5', cnn_trainable=True, lr=LR_MAX_WHOLE, eps=1e-5)\n    if fold == TRAIN_FOLDS[0]:\n        print(model.summary())\n    \n    HISTORIES[fold] = model.fit(\n        train_dataset,\n        steps_per_epoch = train_steps_per_epoch * COMBINE_EPOCHS,\n        validation_data = val_dataset,\n        epochs = EPOCHS_WHOLE,\n        verbose = 2,\n        callbacks = [\n            lr_callback,\n        ],\n    )\n    \n    model.save_weights(f'model_{fold}.h5')\n    \n    print('\\n' * 3)\n    \n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Visualisation\n\nTraining and validation predictions are plotted as a sanity check.","metadata":{}},{"cell_type":"code","source":"def plot_results(dataset, nrows, ncols=4):\n    images, labels = next(iter(dataset))\n    \n    # Predict Masks\n    labels_pred = model(images, training=False)\n    \n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*8, nrows*6))\n    \n    for r, (img, lbl, lbl_pred) in enumerate(zip(images, labels, labels_pred)):\n        if r > nrows - 1: # zero indexed\n            break\n        # Plot Image\n        axes[r, 0].imshow(img)\n        axes[r, 0].set_title('Image', size=18)\n        axes[r, 0].axis(False)\n        \n        # Mask\n        axes[r, 1].imshow(lbl)\n        axes[r, 1].set_title('Mask', size=18)\n        axes[r, 1].axis(False)\n        \n        # Predicted Mask with Threshold\n        axes[r, 2].imshow(lbl_pred)\n        axes[r, 2].set_title('Mask Predicted', size=18)\n        axes[r, 2].axis(False)\n        \n        # Predicted Mask with Threshold\n        lbl_pred_th50 =tf.cast(lbl_pred > 0.50, tf.uint8)\n        axes[r, 3].imshow(lbl_pred_th50)\n        axes[r, 3].set_title('Mask Predicted Threshold 0.50', size=18)\n        axes[r, 3].axis(False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Visualisation\nplot_results(get_dataset(bs=8, idxs=train_idxs), 8)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Validation Visualisation\nplot_results(get_dataset(bs=8, idxs=val_idxs, val=True), 8)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training History\n\nPlot training history metrics to get an understanding on how the model is learning.","metadata":{}},{"cell_type":"code","source":"def plot_history_metric(metric, f_best=np.argmax, ylim=None, yscale=None, yticks=None):\n    plt.figure(figsize=(20, 10))\n    \n    for fold, history in HISTORIES.items():\n        values = history.history[metric]\n        N_EPOCHS = len(values)\n        val = 'val' in ''.join(history.history.keys())\n        # Epoch Ticks\n        if N_EPOCHS <= 20:\n            x = np.arange(1, N_EPOCHS + 1)\n        else:\n            x = [1, 5] + [10 + 5 * idx for idx in range((N_EPOCHS - 10) // 5 + 1)]\n        x_ticks = np.arange(1, N_EPOCHS+1)\n\n        # Validation\n        if val:\n            val_values = history.history[f'val_{metric}']\n            val_argmin = f_best(val_values)\n            plt.plot(x_ticks, val_values, label=f'val_fold_{fold}')\n\n        # summarize history for accuracy\n        plt.plot(x_ticks, values, label=f'train_fold_{fold}')\n        argmin = f_best(values)\n        plt.scatter(argmin + 1, values[argmin], color='red', s=75, marker='o', label=f'train_best_fold_{fold}')\n        if val:\n            plt.scatter(val_argmin + 1, val_values[val_argmin], color='purple', s=75, marker='o', label=f'val_best_fold_{fold}')\n\n    plt.title(f'Model {metric}', fontsize=24, pad=10)\n    plt.ylabel(metric, fontsize=20, labelpad=10)\n\n    if ylim:\n        plt.ylim(ylim)\n\n    if yscale is not None:\n        plt.yscale(yscale)\n        \n    if yticks is not None:\n        plt.yticks(yticks, fontsize=16)\n\n    plt.xlabel('epoch', fontsize=20, labelpad=10)        \n    plt.tick_params(axis='x', labelsize=8)\n    plt.xticks(x, fontsize=16) # set tick step to 1 and let x axis start at 1\n    plt.yticks(fontsize=16)\n    \n    plt.legend(prop={'size': 10})\n    plt.grid()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_history_metric('loss', f_best=np.argmin, yscale='log')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_history_metric('precision', ylim=(0,1), yticks=np.arange(0, 1.1, 0.1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_history_metric('recall', ylim=(0,1), yticks=np.arange(0, 1.1, 0.1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_history_metric('iou', ylim=(0,1), yticks=np.arange(0, 1.1, 0.1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_history_metric('auc', ylim=(0,1), yticks=np.arange(0, 1.1, 0.1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Validation Predictions","metadata":{}},{"cell_type":"code","source":"# Validation Dataset\ndataset = get_dataset(\n        tfrecord_idxs=val_idxs,\n        bs=len(val_idxs),\n        val=True,\n        return_organ=True,\n        debug=True,\n    )\n\n# Validation images, masks and organs\nval_images, val_masks, val_organs = next(iter(dataset))\n# Cast from Tensorflow to Numpy\nval_images = val_images.numpy()\nval_masks = val_masks.numpy()\nval_organs = val_organs.numpy()\nprint(f'val_images shape: {val_images.shape}, val_masks shape: {val_masks.shape}, val_organs shape: {val_organs.shape}')\n\n# Validation Predictions\nVAL_Y_PREDS = model.predict(val_images, verbose=1, batch_size=len(val_idxs))\nprint(f'VAL_Y_PREDS shape: {VAL_Y_PREDS.shape}, VAL_Y_PREDS dtype: {VAL_Y_PREDS.dtype}')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}